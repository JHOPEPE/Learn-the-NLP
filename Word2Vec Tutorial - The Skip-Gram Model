tutorial link:
http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/

中英文摘要

The skip-gram neural network model is actually surprisingly simple in its most basic form; 
I think it’s all of the little tweaks and enhancements that start to clutter the explanation.因为细小的改变和强化，才让解释变得困难。这一点也是skip-gram neural network的难解释点。

We’re going to train a simple neural network with a single hidden layer to perform a certain task, 
but then we’re not actually going to use that neural network for the task we trained it on! 
Instead, the goal is actually just to learn the weights of the hidden layer–we’ll see that these weights are actually the “word vectors” that we’re trying to learn.
这里很trick的一点就是，我们这里用到的neural network的目的，并不是得到一个output，而是希望得到hidden layer中的weights，这个实际上就是我们希望得到的“word vectors”。
【类似的例子】
Another place you may have seen this trick is in unsupervised feature learning, where you train an auto-encoder to compress an input vector in the hidden layer, 
and decompress it back to the original in the output layer. After training it, 
you strip off the output layer (the decompression step) and just use the hidden layer--it's a trick for learning good image features without having labeled training data.

【Fake task】
We’re going to train the neural network to do the following. Given a specific word in the middle of a sentence (the input word), 
look at the words nearby and pick one at random. 
The network is going to tell us the probability for every word in our vocabulary of being the “nearby word” that we chose.我们需要nn（neural network）
告诉我们在我们词汇库中的每一个词，成为我们选择的这个“词A”的“临近词B”的概率。
{near by：When I say "nearby", there is actually a "window size" parameter to the algorithm. 
A typical window size might be 5, meaning 5 words behind and 5 words ahead (10 in total).窗口size，前五个词，后五个词。}
Example：
For example, if you gave the trained network the input word “Soviet”, 
the output probabilities are going to be much higher for words like “Union” and “Russia” than for unrelated words like “watermelon” and “kangaroo”.
Picture link: http://mccormickml.com/assets/word2vec/training_data.png

【Model Details】
First of all, you know you can’t feed a word just as a text string to a neural network, so we need a way to represent the words to the network. 
To do this, we first build a vocabulary of words from our training documents–let’s say we have a vocabulary of 10,000 unique words.
We’re going to represent an input word like “ants” as a one-hot vector. This vector will have 10,000 components (one for every word in our vocabulary) 
and we’ll place a “1” in the position corresponding to the word “ants”, and 0s in all of the other positions.

There is no activation function on the hidden layer neurons, but the output neurons use softmax. We’ll come back to this later.

When training this network on word pairs, the input is a one-hot vector representing the input word 
and the training output is also a one-hot vector representing the output word. But when you evaluate the trained network on an input word, 
the output vector will actually be a probability distribution (i.e., a bunch of floating point values, not a one-hot vector).

【The Hidden Layer】
For our example, we’re going to say that we’re learning word vectors with 300 features. So the hidden layer is going to be represented 
by a weight matrix with 10,000 rows (one for every word in our vocabulary) and 300 columns (one for every hidden neuron).
300 features is what Google used in their published model trained on the Google news dataset (you can download it from here). 
The number of features is a "hyper parameter" 
that you would just have to tune to your application (that is, try different values and see what yields the best results).
If you look at the rows of this weight matrix, these are actually what will be our word vectors!

Now, you might be asking yourself–“That one-hot vector is almost all zeros… what’s the effect of that?” 
If you multiply a 1 x 10,000 one-hot vector by a 10,000 x 300 matrix, 
it will effectively just select the matrix row corresponding to the “1”. Here’s a small example to give you a visual.

This means that the hidden layer of this model is really just operating as a lookup table. 
The output of the hidden layer is just the “word vector” for the input word.
